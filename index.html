<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
        <title>
            Enze Xie's Homepage
        </title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"><!-- Le styles -->
        <link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
        <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css">
        <link href="assets/css/yangqing.css" rel="stylesheet" type="text/css"><!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    </head>
    <body>
        <div class="container">
            <div class="row">
                <div class="span2 bs-docs-sidebar">
                    <hr class="hidden-phone">
                    <div class="text-center hidden-phone">
                        <img src="assets/imgs/enze.png" alt="photo" class="logo-image" width="300" height="300" style="margin: 0px 20px">
                        <!-- <img src="assets/imgs/enze.png" alt="photo" class="logo-image"> -->
                    </div>
                </div>
                <div class="span9">
                    <h3>
                        Enze Xie (谢恩泽)
                    </h3>
                    <h5>
                        <a target="_blank" href="###">CV</a> /
			            <a target="_blank" href="https://github.com/xieenze">GitHub</a> /
                        <a target="_blank" href="https://scholar.google.com/citations?user=42MVVPgAAAAJ&hl=en">Google Scholar</a> /
                        <a target="_blank" href="https://www.zhihu.com/people/xie-en-ze-34/">Zhihu</a> /
			            Email: Johnny_ez@163.com  |  xieenze@hku.hk

                    </h5>
                     <a class="visible-phone pull-left" href="#">
			     <!-- <img class="media-object" src="assets/imgs/enze.png" width="100px" style="margin: 0px 10px"> -->
			</a>
    <p>
    I am a PhD student in Department of Computer Science, <strong>The University of Hong Kong (HKU)</strong> since 2019, supervised by Prof. <a href="http://luoping.me/">Ping Luo</a> and co-supervised by Prof. <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Wenping Wang</a>.
    I also work very close with my friend <a href='https://scholar.google.com/citations?user=WM0OglcAAAAJ&hl=zh-CN'>Wenhai Wang</a> and  Prof. <a href="https://cshen.github.io/">Chunhua Shen</a>.
    I obtained B.S. from Nanjing University of Aeronautics and Astronautics (2016) and M.S. from TongJi University (2019).
    From 2018 to present, I collaborated with several researchers in industry e.g. Face++(Megvii), SenseTime, Facebook(Meta), Huawei and NVIDIA.
	</p>


    <p>
    My research interest is computer vision in 2D/3D. 
    I did some works on instance-level detection and self/semi/weak-supervised learning.
    I developed a few well-known computer vision algorithms including <a href='http://arxiv.org/abs/1909.13226'>PolarMask</a>, which was selected as <a href='https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/'>CVPR 2020 Top-10 Influential Papers</a>. 
    I co-developed  <a href='https://github.com/open-mmlab/OpenSelfSup'>OpenSelfSup(now mmselfsup)</a>(1k+ star), a popular self-supervised learning framework.
	</p>

    <!-- <p>
    <font color="red"> <strong>I am looking for a full-time job or postdoctoral position. 
        Please feel free to contact me  through the email.</strong></font>
    </p> -->




<!--
 *** Publications ***
-->
	<br><br>
    <h3>
        <a name='publications' id="publications"></a> Publications
    </h3>
    (* indicates equal contribution) <br><br>

    <strong>Selected Papers</strong>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/segformer.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</strong>
            </h4>
            <strong>Enze Xie</strong>, Wenhai Wang, Zhiding Yu, Anima Anandkuma, Jose M. Alvarez, Ping Luo<br>
                <i>NeurIPS 2021</i>
            <a href="https://arxiv.org/abs/2105.15203">[paper]</a>
            <a href="https://github.com/NVlabs/SegFormer">[code]</a>
            <a href="https://zhuanlan.zhihu.com/p/379054782">[中文解读]</a>
            <a href="https://www.bilibili.com/video/BV1MV41147Ko/">[demo]</a> 
            <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">[NeurIPS 2021 Top-10 Influential Papers]</a>
            <br> <i><font color="gray"> NVIDIA's first Vision Transformer work and transferred to several product teams.</font></i>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/detco.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>DetCo: Unsupervised Contrastive Learning for Object Detection</strong>
            </h4>
            <strong>Enze Xie*</strong>, Jian Ding*, Wenhai Wang, Xiaohang Zhan, Hang Xu, Zhenguo Li, Ping Luo <br>
                <i>ICCV 2021</i>
            <a href="https://arxiv.org/abs/2102.04803">[paper]</a>
            <a href="https://github.com/xieenze/DetCo">[code]</a>
            <br> <i><font color="gray"> We introduce a detection-friendly unsupervised pre-training solution using large-scale unlabeled data.</font></i>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/pvt2.png" width="150px" height="100px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>PVTv2: Improved Baselines with Pyramid Vision Transformer</strong>
            </h4>
            Wenhai Wang, <strong>Enze Xie</strong>, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao <br>
                <i>Tech report, arXiv</i>
            <a href="https://whai362.github.io/resources/papers/PVTv2_Improved_Baselines_with_Pyramid_Vision_Transformer.pdf">[paper]</a>
            <a href="https://github.com/whai362/PVT">[code]</a>
            <br> <i><font color="gray"> A better version of PVT.</font></i>
        </div>
    </div>
    <br><br>


    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/pvt1.png" width="150px" height="100px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</strong>
            </h4>
            Wenhai Wang, <strong>Enze Xie</strong>, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao <br>
                <i>ICCV 2021</i><font color="red"> (Oral)</font>
            <a href="https://arxiv.org/abs/2102.12122">[paper]</a>
            <a href="https://github.com/whai362/PVT">[code]</a>
            <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">[ICCV 2021 Top-10 Influential Papers]</a>
            <br> <i><font color="gray"> The first work to extend Vision Transformer for object detection and segmentation.</font></i>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/PolarMask.PNG" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>PolarMask: Single Shot Instance Segmentation with Polar Representation</strong>
            </h4>
            <strong>Enze Xie*</strong>, Peize Sun*, Xiaoge Song*, Wenhai Wang, Ding Liang, Chunhua Shen, Ping Luo<br>

			    <i>CVPR 2020 </i><font color="red"> (Oral)</font>
            <a href="http://arxiv.org/abs/1909.13226">[paper]</a>
            <a href="https://github.com/xieenze/PolarMask">[code]</a>
            <a href="https://zhuanlan.zhihu.com/p/84890413">[中文解读]</a>
            <a href="https://www.bilibili.com/video/BV1dp4y1C7Ee?from=search&seid=7560478987246751367">[talk]</a>
            <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">[CVPR20 Top-10 Influential Papers]</a>
            <br> <i><font color="gray"> We introduced a new Polar Representation to reformulate instance segmentation.</font></i>
        </div>
	</div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/polarmask_pp.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</strong>
            </h4>
            <strong>Enze Xie*</strong>, Wenhai Wang*, Mingyu Ding, Ruimao Zhang, Ping Luo<br>
            <i>TPAMI 2021</i>
            <a href="https://arxiv.org/abs/2105.02184">[paper]</a>
            <a href="https://github.com/xieenze/PolarMask">[code]</a> 
            <br> <i><font color="gray"> We extend PolarMask(CVPR'20) to several instance-level detection tasks.</font></i>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/pan_pp.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text</strong>
            </h4>
            Wenhai Wang*, <strong>Enze Xie*</strong>, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu, Chunhua Shen <br>
            <i>TPAMI 2021</i>
            <a href="https://arxiv.org/abs/2105.00405">[paper]</a>
            <a href="https://github.com/whai362/pan_pp.pytorch">[code]</a> 
            <br> <i><font color="gray"> We extend PSENet (CVPR'19) and PAN (ICCV'19) to a text spotting system.</font></i>
        </div>
    </div>
    <br><br>

    <strong>Other Papers</strong>


    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Improving Monocular Visual Odometry Using Learned Depth</strong>
            </h4>
            Libo Sun, Wei Yin, <strong>Enze Xie</strong>,  Zhengrong Li, Changming Sun, Chunhua Shen <br>
                <i>IEEE Transactions on Robotics 2022</i>
                <a href="https://arxiv.org/abs/2204.01268">[paper]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/dupr.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Deeply Unsupervised Patch Re-Identification for Pre-training Object Detectors</strong>
            </h4>
            Jian Ding*, <strong>Enze Xie*</strong>, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo, Gui-Song Xia <br>
                <i>TPAMI 2022</i>
                <a href="https://arxiv.org/abs/2103.04814">[paper]</a>
                <a href="https://github.com/dingjiansw101/DUPR">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/panoseg.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</strong>
            </h4>
            Zhiqi Li, Wenhai Wang, <strong>Enze Xie</strong>, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Tong Lu, Ping Luo <br>
                <i>CVPR 2022</i>
            <a href="https://arxiv.org/abs/2109.03814">[paper]</a>
            <a href="https://github.com/zhiqi-li/Panoptic-SegFormer">[code]</a>
           
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/cyclemlp.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>CycleMLP: A MLP-like Architecture for Dense Prediction</strong>
            </h4>
            Shoufa Chen, <strong>Enze Xie</strong>, Chongjian Ge, Runjian Chen, Ding Liang, Ping Luo <br>
                <i>ICLR 2022</i><font color="red"> (Oral)</font>
            <a href="https://arxiv.org/abs/2107.10224">[paper]</a>
            <a href="https://github.com/ShoufaChen/CycleMLP">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/tin.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization</strong>
            </h4>
            Zhe Chen, Wenhai Wang, <strong>Enze Xie</strong>, Tong Lu, Ping Luo <br>
                <i>AAAI 2022</i>
            <a href="https://arxiv.org/abs/2103.11784">[paper]</a>
            <a href="https://github.com/czczup/URST">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/woo.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Watch Only Once: An End-to-End Video Action Detection Framework</strong>
            </h4>
            Shoufa Chen, Peize Sun, <strong>Enze Xie</strong>, Chongjian Ge, Jiannan Wu, Lan Ma, Jiajun Shen, Ping Luo <br>
                <i>ICCV 2021</i>
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf">[paper]</a>
            <a href="https://github.com/ShoufaChen/WOO">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/onenet_icml.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>What Makes for End-to-End Object Detection?</strong>
            </h4>
            Peize Sun, Yi Jiang, <strong>Enze Xie</strong>, Wenqi Shao, Zehuan Yuan, Changhu Wang, Ping Luo <br>
                <i>ICML 2021</i>
            <a href="http://proceedings.mlr.press/v139/sun21b/sun21b.pdf">[paper]</a>
            <a href="https://github.com/PeizeSun/OneNet">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/trans2seg.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Segmenting Transparent Objects in the Wild with Transformer</strong>
            </h4>
            <strong>Enze Xie</strong>, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, Ping Luo <br>
                <i>IJCAI 2021</i>
            <a href="https://arxiv.org/abs/2101.08461">[paper]</a>
            <a href="https://github.com/xieenze/Trans2Seg">[code & dataset]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/TransLab.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Segmenting Transparent Objects in the Wild</strong>
            </h4>
            <strong>Enze Xie</strong>, Wenjia Wang, Wenhai Wang, Mingyu Ding, Chunhua Shen, Ping Luo <br>
                <i>ECCV 2020</i>
            <a href="https://arxiv.org/abs/2003.13948">[paper]</a>
            <a href="https://github.com/xieenze/Segment_Transparent_Objects">[code & dataset]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/stsr.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Scene Text Image Super-Resolution in the Wild</strong>
            </h4>
            Wenjia Wang*, <strong>Enze Xie*</strong>, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua Shen, Xiang Bai <br>
                <i>ECCV 2020</i>
            <a href="https://arxiv.org/abs/2005.03341">[paper]</a>
            <a href="https://github.com/JasonBoy1/TextZoom">[code & dataset]</a>
        </div>
    </div>
    <br><br>


    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/gg.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Differentiable Hierarchical Graph Grouping for Multi-Person Pose Estimation</strong>
            </h4>
            Sheng Jin, Wentao Liu, <strong>Enze Xie</strong>, Wenhai Wang, Chen Qian, Wanli Ouyang, Ping Luo <br>
                <i>ECCV 2020</i>
            <a href="https://arxiv.org/abs/2007.11864">[paper]</a>
            <!-- <a href="">[Project Web]</a> -->
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/AE_TextSpotter.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting</strong>
            </h4>
            Wenhai Wang, Xuebo Liu, Xiaozhong Ji, <strong>Enze Xie</strong>, Ding Liang, ZhiBo Yang, Tong Lu, Chunhua Shen, Ping Luo <br>
                <i>ECCV 2020</i>
            <a href="https://arxiv.org/abs/2008.00714">[paper]</a>
            <a href="https://github.com/whai362/TDA-ReCTS">[Project Web]</a>
        </div>
    </div>
    <br><br>


    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/pan.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network</strong>
            </h4>
              Wenhai Wang*, <strong>Enze Xie*</strong>,  Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu, Gang Yu, Chunhua Shen <br>
              <i>ICCV 2019</i>
            <a href="https://arxiv.org/abs/1908.05900">[paper]</a>
            <a href="https://github.com/whai362/pan_pp.pytorch">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/psenet.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong>
            </h4>
              Wenhai Wang*, <strong>Enze Xie*</strong>,  Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao <br>
              <i>CVPR 2019</i>
            <a href="https://arxiv.org/abs/1903.12473">[paper]</a>
            <a href="https://github.com/whai362/PSENet">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/spcnet.png" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Scene Text Detection with Supervised Pyramid Context Network</strong>
            </h4>
              <strong>Enze Xie*</strong>, Yuhang Zang*, Shuai Shao, Gang Yu, Cong Yao, Guangyao Li <br>
              <i>AAAI 2019</i>
            <a href="https://arxiv.org/abs/1811.08605">[paper]</a>
        </div>
    </div>
    <br><br>



    <strong> Technical Report</strong><br>


    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/dupr.png" width="150px" height="120px"></a>


    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/transtrack.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>TransTrack: Multiple-Object Tracking with Transformer</strong>
            </h4>
            Peize Sun, Yi Jiang, Rufeng Zhang, <strong>Enze Xie</strong>, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, Ping Luo <br>
                <i>Tech report, arXiv</i>
            <a href="https://arxiv.org/abs/2012.15460">[paper]</a>
            <a href="https://github.com/PeizeSun/TransTrack">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/onenet.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>OneNet: Towards End-to-End One-Stage Object Detection</strong>
            </h4>
            Peize Sun, Yi Jiang, <strong>Enze Xie</strong>, Zehuan Yuan, Changhu Wang, Ping Luo <br>
                <i>Tech report, arXiv</i>
            <a href="https://arxiv.org/abs/2012.05780">[paper]</a>
            <a href="https://github.com/PeizeSun/OneNet">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/sbp.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>SelfText Beyond Polygon: Unconstrained Text Detection with Box Supervision and Dynamic Self-Training</strong>
            </h4>
            Weijia Wu*, <strong>Enze Xie*</strong> , Ruimao Zhang, Wenhai Wang, Guan Pang, Zhen Li, Hong Zhou, Ping Luo <br>
                <i>Tech report, arXiv</i>
            <a href="https://arxiv.org/abs/2011.13307">[paper]</a>
            <a href="https://github.com/weijiawu/Unconstrained-Text-Detection-with-Box-Supervisionand-Dynamic-Self-Training">[code]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/oi_seg.png" width="150px" height="120px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>1st Place Solutions for OpenImage2019--Object Detection and Instance Segmentation</strong>
            </h4>
                Yu Liu, Guanglu Song, Yuhang Zang, Yan Gao, <strong>Enze Xie</strong>, Junjie Yan, Chen Change Loy, Xiaogang Wang <br>
                <i>Tech report, arXiv</i>
            <a href="https://arxiv.org/abs/2003.07557">[paper]</a>
        </div>
    </div>
    <br><br>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/textsr.PNG" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>TextSR: Content-Aware Text Super-Resolution Guided by Recognition</strong>
            </h4>
              Wenjia Wang*, <strong>Enze Xie*</strong>, Peize Sun, Wenhai Wang, Lixun Tian, Chunhua Shen, Ping Luo <br>
              <i>Tech report, arXiv</i>
            <a href="https://arxiv.org/abs/1909.07113">[paper]</a>
            <a href="https://github.com/xieenze/TextSR">[code]</a> <br>
            <i><font color="black"> Improved version has been accepted by ECCV2020</font></i>
        </div>
    </div>


<!-- *** Experience *** -->
<!-- <br><br>
<h3>
    <a name='Experience'></a> Experience
</h3>

    <div style="float:left; text-align:left"><a href='https://www.nvidia.com/en-us/research/'>NVIDIA Research</a></div>
    <div style="float:right; text-align:right">2021.03 – Now</div> <br />
    Research Intern<br />
     working on 3D detection->tracking->forecasting in autonomous driving with
     <a href="https://chrisding.github.io/">Zhiding Yu</a>, 
     <a href="https://scholar.google.com/citations?user=Oyx-_UIAAAAJ&hl=zh-CN">Jose M. Alvarez</a>,
    <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>,
     and <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> 
     <br /><br />

    <div style="float:left; text-align:left">AI Theory Group, <a href='https://www.noahlab.com.hk/#/home'>HUAWEI Noah's Ark Lab</a></div>
    <div style="float:right; text-align:right">2020.06 – 2021.02</div> <br />
    Research Intern<br />
     working on self-supervised learning and Transformer for dense prediction with 
     <a href="https://scholar.google.com/citations?user=J_8TX6sAAAAJ&hl=en">Hang Xu</a> and <a href="https://www.ee.columbia.edu/~zgli/">Zhenguo Li</a> 
     <br /><br />

    <div style="float:left; text-align:left">Apply Machine Learning (AML) Team, <a href='https://ai.facebook.com/'>Facebook AI</a></div>
    <div style="float:right; text-align:right">2020.05 – 2020.07</div> <br />
    <s>Research Intern</s> -> Project Co-Operator (Due to COVID19)<br />
     working on weak and semi-supervised OCR with <a href="https://scholar.google.com/citations?user=7v1LZxUAAAAJ&hl=en">Guan Pang</a> <br /><br />

    <div style="float:left; text-align:left">General Model Team, <a href='https://www.sensetime.com/'>SenseTime Research</a></div>
    <div style="float:right; text-align:right">2019.07 – 2020.03</div> <br />
    Research Intern<br />
     working on instace-level detection with <a href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ&hl=en">Ding Liang</a> <br /><br />

    <div style="float:left; text-align:left">Detection Team, <a href="https://https://megvii.com/">Megvii(Face++) Research</a></div>
    <div style="float:right; text-align:right">2018.04 – 2019.07</div> <br />
    Research Intern<br />
     working on OCR and instance-level detection with <a href="http://www.skicyyu.org/">Gang Yu</a> -->

<!-- *** Challenges *** -->
<br><br>
<h3>
    <a name='Challenges'></a> Challenges
</h3>

<div style="float:left; text-align:left">Rank 1 in <a href="https://naic.pcl.ac.cn/api/backstatic/%E5%86%B3%E8%B5%9B%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95%E5%85%AC%E5%B8%83-2020%E5%B9%B4%E5%85%A8%E5%9B%BD%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%A7%E8%B5%9B.pdf"> National Artificial Intelligence Competition - Remote Sensing Segmentation</a> <i><font color="red"> (bonus 100,0000 RMB)</font></i></div>
<div style="float:right; text-align:right">2020</div> <br />

<div style="float:left; text-align:left">Rank 1 in <a href="https://www.kaggle.com/c/open-images-2019-instance-segmentation/leaderboard"> Google Open Images 2019 - Instance Segmentation</a></div>
<div style="float:right; text-align:right">2019</div> <br />

<div style="float:left; text-align:left">Rank 1 in <a href="https://rrc.cvc.uab.es/?ch=14&com=evaluation&task=1">ICDAR 2019 Arbitrary-Shaped Text Detection</a></div>
<div style="float:right; text-align:right">2019</div> <br />

<div style="float:left; text-align:left">Rank 2 in  <a href="https://rrc.cvc.uab.es/?ch=16&com=evaluation&task=1">ICDAR 2019 Large-scale Street View Text Detection  </a></div>
<div style="float:right; text-align:right">2019</div>



<!-- *** Professional activities *** -->
<br><br>
<h3>
    <a name='Professional Activities'></a> Professional Activities
</h3>

<div style="float:left; text-align:left">Organizer for <a href="https://sites.google.com/view/t4v-cvpr22">T4V: Transformers for Vision Workshop</a> at CVPR 2022</div>  <br />
<div style="float:left; text-align:left">Journal Reviewer for TPAMI, IJCV, RA-L, T-MM</div> <br />
<div style="float:left; text-align:left">Conference Reviewer for ICML, NeurIPS, CVPR, ICCV, ECCV, IROS, IJCAI, AAAI, WACV, ACCV </div> <br />

<div style="float:left; text-align:left">SPC for IJCAI21,22</div>

<!-- *** Presentation *** -->
<br><br>
<h3>
    <a name='Presentation'></a> Invited Talks
</h3>

<div style="float:left; text-align:left">Stanford MedAI : <a target="_blank" href="https://www.youtube.com/watch?v=Yf9fNn1fWy8">"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"</a></div>
<div style="float:right; text-align:right">2021</div> <br />

<div style="float:left; text-align:left">Huawei Noah's Ark Lab - AI Theory Group : <a target="_blank" href="huawei_talk_XieEnze.pdf">"Instance Level Detection and Beyond"</a></div>
<div style="float:right; text-align:right">2021</div> <br />

<div style="float:left; text-align:left">SenseTime : <a target="_blank" href="ssl_present_st.pdf">"Self-Supervised Learning for Classification and Beyond"</a></div>
<div style="float:right; text-align:right">2020</div> <br />

<div style="float:left; text-align:left">Microsoft Research Asia (MSRA) VCG : <a target="_blank" href="MSRA_Talk_PolarMask.pdf">"Polar Representation in Instance Segmentation"</a></a></div>
<div style="float:right; text-align:right">2020</div>

<div style="float:left; text-align:left">Hong Kong Computer Vision Workshop(HKCVW) : <a target="_blank" href="HKCVW_Oral_xieenze_PAN.pdf">"Real-Time Scene Text Detection"</a></div>
<div style="float:right; text-align:right">2019</div> <br />

<!-- *** Honours and Awards *** -->
<br><br>
<h3>
    <a name='Honours and Awards'></a> Honours and Awards
</h3>

<div style="float:left; text-align:left">NVIDIA Graduate Fellowship Finalist Award (The first candidate from Chinese University in 21 years)</div>
<div style="float:right; text-align:right">2022</div> <br />

<div style="float:left; text-align:left">NeurIPS 2021 Outstanding Reviewer Award (top 8% of reviewers)</div>
<div style="float:right; text-align:right">2021</div> <br />

<div style="float:left; text-align:left">Hong Kong and China Gas Company Limited Postgraduate Prize</div>
<div style="float:right; text-align:right">2021</div> <br />

<div style="float:left; text-align:left">Outstanding Master Thesis Award, Tongji University</div>
<div style="float:right; text-align:right">2019</div> <br />


<!-- <hr> -->
<div class="container">
<div class="row">
<div class="span12">
<p>
    <h3>
        <a name='Some of My Friends'></a> Some of my Friends
    </h3>
        <a target="_blank" href="https://whai362.github.io/"> Wenhai Wang</a> (NJU), 
        <a target="_blank" href="https://scholar.google.com/citations?user=cVWmlYQAAAAJ&hl=zh-CN"> Wenjia Wang</a>  (SenseTime),
        <a target="_blank" href="http://wangjingbo.top/"> Jingbo Wang</a>  (CUHK),
        <a target="_blank" href="https://xiaohangzhan.github.io/"> Xiaohang Zhan</a>  (CUHK),
        <a target="_blank" href="https://scholar.google.com/citations?user=7v1LZxUAAAAJ&hl=en"> Guan Pang</a>  (Facebook AI),
        <a target="_blank" href="https://cshen.github.io/"> Chunhua Shen</a>  (Uni Adelaide) 
</p>
</div>
</div>
</div>

</div>
</div>
</div>
</body>
</html>
