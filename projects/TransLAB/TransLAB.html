

<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Segmenting Transparent Objects in the Wild </title>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:30px">Segmenting Transparent Objects in the Wild</span>
	</center>

	<br>


  	<table align=center width=900px>
  	 <tr>
		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=42MVVPgAAAAJ&hl=en">Enze Xie</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=cVWmlYQAAAAJ&hl=zh-CN">Wenjia Wang</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=WM0OglcAAAAJ&hl=en">Wenhai Wang</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px">Mingyu Ding</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ&hl=en">Chunhua Shen</a></span>
		</center>
        </td>

        <td align=center width=80px>
            <center>
            <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=aXdjxb4AAAAJ&hl=en">Ping Luo</a></span>
            </center>
        </td>

	 </tr>
	</table>

	<br>


	<table align=center width=1100px>
  	 <tr>
        <td align=center width=120px>
            <center>
            <span style="font-size:20px">The University of Hong Kong</span>
            </center>
        </td>

		<td align=center width=80px>
		<center>
		<span style="font-size:20px">SenseTime Research</span>
		</center>
		</td>


		<td align=center width=80px>
		<center>
		<span style="font-size:20px">Nanjing University</span>
		</center>
        </td>

        <td align=center width=120px>
            <center>
            <span style="font-size:20px">The University of Adelaide</span>
            </center>
        </td>
	 </tr>
	</table>
	<br>
	<table align=left width=1100px>
		<tr>
		 <td align=left width=120px>
			 <left>
				&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
				<span style="font-size:15px">Any questions are welcome to email Xie Enze: xieenze@hku.hk</span>
			 </left>
		 </td>

	  </tr>
	 </table>

	<br>
	<hr>
	<table align=center width=1100px>
		<tr>
			<td>
			<left>
		<left><h3>News</h3></left>
		<p class="lead"><font color="red">[news]</font> (2020.09.12) We have released the code and the test data. </p>
		<p class="lead"><font color="red">[news]</font> (2020.07.03) The paper is accepted by ECCV 2020, we'll release the rest data and code soon. </p>
		<p class="lead"><font color="red">[news]</font> (2020.04.10) We have released the Train and Val set of Trans10K dataset. </p>
	</left>
	</td>
	</tr>
	</table>
	<hr>
	<br>



  		  <table align=center width=900px>
  			  <tr>
  	              <td width=900px>
  					<center>
  	                	<a href="./support/Trans10K.png"><img src = "./support/Trans10K.png" height="500px"></img></href></a><br>
					</center>
  	              </td>
  	          </tr>
  		  </table>

      	  <br>
      	  <p style="text-align:justify">
            Transparent objects such as windows and bottles made by glass widely exist in the real world. Segmenting transparent objects is challenging because these objects have diverse appearance inherited from the image background, making them had similar appearance with their surroundings. Besides the technical difficulty of this task, only a few previous datasets were specially designed and collected to explore this task and most of the existing datasets have major drawbacks. They either possess limited sample size such as merely a thousand of images without manual annotations, or they generate all images by using computer graphics method (i.e. not real image). To address this important problem, this work proposes a large-scale dataset for transparent object segmentation, named Trans10K, consisting of 10,428 images of real scenarios with carefully manual annotations, which are 10 times larger than the existing datasets. The transparent objects in Trans10K are extremely challenging due to high diversity in scale, viewpoint and occlusion as shown in Fig. 1. To evaluate the effectiveness of Trans10K, we propose a novel boundary-aware segmentation method, termed TransLab, which exploits boundary as the clue to improve segmentation of transparent objects. Extensive experiments and ablation studies demonstrate the effectiveness of Trans10K and validate the practicality of learning object boundary in TransLab. For example, TransLab significantly outperforms 20 recent object segmentation methods based on deep learning, showing that this task is largely unsolved. We believe that both Trans10K and TransLab have important contributions to both the academia and industry, facilitating future researches and applications.
      	  </p>
  		  <br>
		  <hr>
		 <!-- <table align=center width=550px> -->
  		  <table align=center width=1100>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
				  <td><a href="arxiv"><img style="height:180px" src="./support/paper.png"/></a></td>
				  <td><span style="font-size:14pt"></span><a href="https://arxiv.org/pdf/2003.13948.pdf">Segmenting Transparent Objects in the Wild</a><br><br>
                    <i>Enze Xie, Wenjia Wang, Wenhai Wang, Mingyu Ding, Chunhua Shen, Ping Luo</i><br><br>
					Tech Report
					</td>
					</td>
              </tr>
  		  </table>

		<br>
  		<hr>

  		  <table align=center width=1100>
	 		<center><h1>Data</h1></center>
	 		<center>
		   	<div>
				<img src="./support/images.png" height="700">
				<!-- <img src="./support/cat.jpg" height="170" width=170>
				<img src="./support/horse.jpg" height="170" width=170>
				<img src="./support/giraffe.jpg" height="170" width=170>
				<img src="./support/car.jpg" height="170" width=170>
				<img src="./support/bus.jpg" height="170" width=170> -->
		 	</div>
		 	</center>
			<p class="text-justify">
				The Trans10K dataset contains 10,428 images, with two categories of transparent
				objects: (1) Transparent things such as cups, bottles and glass, locating these
				things can make robots easier to grab objects. (2) Transparent stuff such as
				windows, glass walls and glass doors. It can make robots learn to avoid obstacles
				and avoid hitting these stuff. 5000, 1000 and 4428 images
				are used for train, validation and test, respectively.
				We further divide the validation set and test set into two parts, easy and hard according to the difficulty.
			</p>
			</table>

			<div>
					<left><h3>Download Dataset</h3></left>
					<left>Train Set:
						 			 <a href="https://drive.google.com/file/d/1ZNTTbGjvPUHuLj3r3wEg8m12et7tDluz/view?usp=sharing">[Google Drive]</a>,
									 <a href="https://pan.baidu.com/s/15XBr1zfnuNc8fA3YwXRqAA">[Baidu Drive] </a>Fetch Code: axd1  </left> <br>
					<left>Val Set:&nbsp;&nbsp;&nbsp;
									 <a href="https://drive.google.com/file/d/1I3BVEygFrCT3BmZL9byt2VFMt5ouCwRX/view?usp=sharing">[Google Drive]</a>,
									 <a href="https://pan.baidu.com/s/1SzJZfvof1iRkZLvK5WL0Ow">[Baidu Drive]</a> Fetch Code: bsbd</left> <br>
					<left>Test Set: &nbsp;
									 <a href="https://drive.google.com/file/d/1A1mLgpOjioSQ1pzSUkU0Ce_3W_YozqgY/view?usp=sharing">[Google Drive]</a>,
								 	 <a href="https://pan.baidu.com/s/18FQRL4uD84D8O2ORdMGXBA">[Baidu Drive]</a> Fetch Code: hiqi</left> <br>
							 </left>
			</div>
		 	<br>

		  <!-- 分界线  引用 -->
		  <hr/>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  				<left>
				<center><h1>BibTex</h1></center>
	  		  @article{xie2020segmenting, <br>
				&nbsp;&nbsp;&nbsp;&nbsp; title={Segmenting Transparent Objects in the Wild}, <br>
				&nbsp;&nbsp;&nbsp;&nbsp; author={Xie, Enze and Wang, Wenjia and Wang, Wenhai and Ding, Mingyu and Shen, Chunhua and Luo, Ping}, <br>
				&nbsp;&nbsp;&nbsp;&nbsp; journal={arXiv preprint arXiv:2003.13948},<br>
				&nbsp;&nbsp;&nbsp;&nbsp; year={2020}<br>
			  }

			</left>
		</td>
		</tr>
		</table>
		<br>




		  <!-- 分界线 -->
  		  <hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
	  		  We would like to thank Yichao Xu in A*STAR for insightful discussion.
			</left>
		</td>
		</tr>
		</table>
		<br>
</body>
</html>
